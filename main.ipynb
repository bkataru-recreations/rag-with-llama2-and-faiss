{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install markdown\n",
    "%pip install langchain\n",
    "%pip install pdfminer.six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import markdown\n",
    "from pdfminer.high_level import extract_text as extract_text_from_pdf\n",
    "from io import StringIO\n",
    "from html.parser import HTMLParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# HTML Stripper class to clean HTML tags from the text\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs = True\n",
    "        self.text = StringIO()\n",
    "\n",
    "    def handle_data(self, d):\n",
    "        self.text.write(d)\n",
    "    \n",
    "    def get_data(self):\n",
    "        return self.text.getvalue()\n",
    "\n",
    "def strip_tags(html):\n",
    "    \"\"\"Remove HTML tags from a string.\"\"\"\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()\n",
    "\n",
    "def clean_markdown(text):\n",
    "    \"\"\"Clean Markdown syntax from text.\"\"\"\n",
    "\n",
    "    # Remove Markdown URL links\n",
    "    text = re.sub(r'\\[([^\\]]+)\\]\\([^)]+\\)', r'\\1', text)\n",
    "    \n",
    "    # Remove bold and italic text markers\n",
    "    text = re.sub(r'\\*\\*([^*]+)\\*\\*', r'\\1', text)\n",
    "    text = re.sub(r'\\*([^*]+)\\*', r'\\1', text)\n",
    "    text = re.sub(r'__([^_]+)__', r'\\1', text)\n",
    "    text = re.sub(r'_([^_]+)_', r'\\1', text)\n",
    "\n",
    "    # Remove images and their references\n",
    "    text = re.sub(r'!\\[[^\\]]]*]\\([^)]*\\)', '', text)\n",
    "\n",
    "    # Remove header markers\n",
    "    text = re.sub(r'#+\\s?', '', text)\n",
    "\n",
    "    # Remove other Markdown syntax as needed (e.g., tables, bullet points)\n",
    "    text = re.sub(r'\\|', ' ', text)\n",
    "    text = re.sub(r'-{2,}', '', text)\n",
    "    text = re.sub(r'\\n{2,}', '\\n', text)  # Remove extra newlines\n",
    "\n",
    "    return text\n",
    "\n",
    "def extract_text_from_md(md_path):\n",
    "    \"\"\"Extract and clean text from a Markdown file.\"\"\"\n",
    "\n",
    "    with open(md_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        md_content = file.read()\n",
    "        html = markdown.markdown(md_content)\n",
    "        text = strip_tags(html)\n",
    "\n",
    "        return clean_markdown(text)\n",
    "\n",
    "def extract_text_from_file(file_path):\n",
    "    \"\"\"Extract text from a file based on its extension.\"\"\"\n",
    "\n",
    "    if file_path.endswith('.pdf'):\n",
    "        return extract_text_from_pdf(file_path)\n",
    "    elif file_path.endswith('.md'):\n",
    "        return extract_text_from_md(file_path)\n",
    "    elif file_path.endswith('.txt'):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "    else:\n",
    "        return \"Unsupported file format.\"\n",
    "\n",
    "# Directory containing documents to process\n",
    "directory = r'./corpus/'\n",
    "\n",
    "# Parameters for text splitting\n",
    "chunk_size = 1200\n",
    "chunk_overlap = 300\n",
    "\n",
    "# List to store all document chunks\n",
    "all_docs = []\n",
    "allowed_extensions = ['.md', '.pdf', '.txt']\n",
    "\n",
    "# Process each file in the directory\n",
    "for root, dirs, files in os.walk(directory):\n",
    "    for filename in files:\n",
    "        # Get the file extension\n",
    "        _, file_extension = os.path.splitext(filename)\n",
    "        if file_extension in allowed_extensions:\n",
    "            file_path = os.path.join(root, filename) # Full path of the file\n",
    "\n",
    "            # Remove the \".md\", \".pdf\", or \".txt\" extension from the file name\n",
    "            file_name_without_extension = os.path.splitext(filename)[0]\n",
    "\n",
    "            # Open and read the file\n",
    "            file_content = extract_text_from_file(file_path)\n",
    "\n",
    "            # Split the text into chunks\n",
    "            text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "            docs = text_splitter.split_text(file_content)\n",
    "\n",
    "            for i, chunk in enumerate(docs):\n",
    "                # Define metadata for each chunk (can customize this)\n",
    "\n",
    "                metadata = {\n",
    "                    \"File Name\": file_name_without_extension,\n",
    "                    \"Chunk Number\": i + 1,\n",
    "                }\n",
    "\n",
    "                # Create a header with metadata and file name\n",
    "                header = f\"File Name: {file_name_without_extension}\\n\"\n",
    "                for key, value in metadata.items():\n",
    "                    header += f\"{key}: {value}\\n\"\n",
    "\n",
    "                # Combine header, file name, and chunk content\n",
    "                chunk_with_header = header + file_name_without_extension + \"\\n\" + chunk\n",
    "\n",
    "                all_docs.append(chunk_with_header)\n",
    "\n",
    "            print(f\"Processed: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "\n",
    "# Initialize HuggingFaceInstructEmbeddings\n",
    "model_name = \"hkunlp/instructor-large\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "hf_embedding = HuggingFaceInstructEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "# Embed and index all the documents using FAISS\n",
    "db = FAISS.from_texts(all_docs, hf_embedding)\n",
    "\n",
    "# Save the indexed data locally\n",
    "db.save_local(\"faiss_AiDoc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the FAISS index from local storage\n",
    "db = FAISS.load_local(\"faiss_AiDoc\", embeddings=hf_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -LO https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/blob/main/llama-2-7b-chat.Q6_K.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "# Template for question-answer prompt\n",
    "template = \"\"\"Question: {question}\\n\\nAnswer:\"\"\"\n",
    "\n",
    "# Initialize prompt template and callback manager\n",
    "prompt = PromptTemplate(template=template, input_variable=[\"question\"])\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "# Define the local path to the Llama2 model download\n",
    "model_path = \"llama-2-7b-chat.Q6_K.gguf\"\n",
    "\n",
    "# Initialize LlamaCpp model\n",
    "llm = LlamaCpp(\n",
    "    model_path=model_path, \n",
    "    temperature=0.2, \n",
    "    max_tokens=4095,\n",
    "    top_p=1,\n",
    "    callback_manager=callback_manager,\n",
    "    n_ctx=6000\n",
    ")\n",
    "\n",
    "# Create LLMChain\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "# Define a query to search the indexed documents\n",
    "query = \"What is the state of current research in quantum gravity?\"\n",
    "\n",
    "# Search for semantically similar chunks and return the top 5 chunks\n",
    "search = db.similarity_search(query, k=5)\n",
    "\n",
    "# Define a template for generating a final prompt\n",
    "template = '''Context: {context}\n",
    "Based on the Context, please answer the following question:\n",
    "Question: {question}\n",
    "Provide an answer based on the context only, without using general knowledge. The answer should be derived directly from the context provided.\n",
    "Please correct any grammatical errors for improved readability.\n",
    "If the context does not contain relevant information to answer the question, state that the answer is not available in the given context.\n",
    "Please include the source title of the information as a reference of how you arrive at your answer.'''\n",
    "\n",
    "# Create a prompt template\n",
    "prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template=template)\n",
    "\n",
    "# Format the final prompt with the query and search results\n",
    "final_prompt = prompt.format(question=query, context=search)\n",
    "\n",
    "# Run LLMChain to generate an answer based on the context\n",
    "llm_chain.run(final_prompt)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
